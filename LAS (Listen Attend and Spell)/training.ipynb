{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '\"', '#', ..., '火礮', '\\ufeff', '～'], dtype='<U20')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all that we need\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy')\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')\n",
    "vocab\n",
    "# print(vocab.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # self.indexes = np.arange(len(dataset))\n",
    "        self.seq_len = np.array([len(st) for st in self.dataset])\n",
    "        self.mean = np.mean(self.seq_len)\n",
    "        self.var = np.var(self.seq_len)\n",
    "\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        pack = self.dataset.copy()\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(len(pack))\n",
    "        else:\n",
    "            self.indexes = np.arange(len(pack))\n",
    "        pack = np.concatenate(pack[self.indexes])\n",
    "        truncate = len(pack) % self.batch_size\n",
    "        pack = pack[:-truncate]\n",
    "        pack = pack.reshape(self.batch_size, -1)\n",
    "        \n",
    "        self.X = pack[:,:-1] ; self.Y = pack[:, 1:]\n",
    "        idx = 0\n",
    "        while True:\n",
    "            if np.random.binomial(1, 0.95):\n",
    "                s = int(np.random.normal(70, 5))\n",
    "            else:\n",
    "                s = int(np.random.normal(35, 5))\n",
    "            if idx + s > self.X.shape[1]:\n",
    "                yield (torch.from_numpy(self.X[:, idx:].transpose()).long(), torch.from_numpy(self.Y[:, idx:].transpose()).long())\n",
    "                break\n",
    "            else:\n",
    "                yield (torch.from_numpy(self.X[:, idx:idx+s].transpose()).long(), torch.from_numpy(self.Y[:, idx:idx+s].transpose()).long())\n",
    "                idx += s\n",
    "\n",
    "                \n",
    "# train_loader = LanguageModelDataLoader(dataset, 80)\n",
    "# for e in range(10):\n",
    "#     for i, (inputs,targets) in enumerate(train_loader):\n",
    "#         print(inputs.size(), targets.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDrop(nn.Module):\n",
    "    \"\"\"Implenmentation from salesforce\"\"\"\n",
    "    \n",
    "    def __init__(self, module, weights, dropout=0, variational=False):\n",
    "        super(WeightDrop, self).__init__()\n",
    "        self.module = module\n",
    "        self.weights = weights\n",
    "        self.dropout = dropout\n",
    "        self.variational = variational\n",
    "        self._setup()\n",
    "\n",
    "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
    "        # We need to replace flatten_parameters with a nothing function\n",
    "#         It must be a function rather than a lambda as otherwise pickling explodes\n",
    "        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n",
    "        # (╯°□°）╯︵ ┻━┻\n",
    "        return\n",
    "\n",
    "    def _setup(self):\n",
    "        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n",
    "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
    "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
    "\n",
    "        for name_w in self.weights:\n",
    "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
    "            w = getattr(self.module, name_w)\n",
    "            del self.module._parameters[name_w]\n",
    "            self.module.register_parameter(name_w + '_raw', nn.Parameter(w.data))\n",
    "\n",
    "    def _setweights(self):\n",
    "        for name_w in self.weights:\n",
    "            raw_w = getattr(self.module, name_w + '_raw')\n",
    "            w = None\n",
    "            if self.variational:\n",
    "                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n",
    "                if raw_w.is_cuda: mask = mask.cuda()\n",
    "                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n",
    "                w = nn.Parameter(mask.expand_as(raw_w) * raw_w)\n",
    "            else:\n",
    "                w = nn.Parameter(torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training))\n",
    "            setattr(self.module, name_w, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if dropout == 0 or not self.training:\n",
    "            return x \n",
    "        mask = x.data.new(x.size(0), 1, x.size(2))\n",
    "        mask = mask.bernoulli_(1 - dropout)\n",
    "        mask = Variable(mask, requires_grad=False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
    "    if dropout:\n",
    "        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
    "        masked_embed_weight = mask * embed.weight\n",
    "    else:\n",
    "        masked_embed_weight = embed.weight\n",
    "    if scale:\n",
    "        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "    padding_idx = embed.padding_idx\n",
    "    if padding_idx is None:\n",
    "        padding_idx = -1\n",
    "\n",
    "    X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "    padding_idx, embed.max_norm, embed.norm_type,\n",
    "    embed.scale_grad_by_freq, embed.sparse\n",
    "    )\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size=400, hidden_size=1150, n_layers=3, tie_weights=False):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.tie_weights = tie_weights\n",
    "        \n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnns = [nn.LSTM(embedding_size if l == 0 else hidden_size, hidden_size \\\n",
    "                    if l != n_layers-1 else (embedding_size if tie_weights else hidden_size), 1, dropout=0)for l in range(n_layers)]\n",
    "        self.rnns = [WeightDrop(rnn, ['weight_hh_l0'], dropout=0.5) for rnn in self.rnns]\n",
    "        print(self.rnns)\n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        self.decoder = nn.Linear(embedding_size if tie_weights else hidden_size, vocab_size)\n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return [(weight.new(1, batch_size, self.hidden_size if l != self.n_layers - 1 else (self.embedding_size if self.tie_weights else self.hidden_size)).zero_(),\n",
    "                    weight.new(1, batch_size, self.hidden_size if l != self.n_layers - 1 else (self.embedding_size if self.tie_weights else self.hidden_size)).zero_())\n",
    "                    for l in range(self.n_layers)]\n",
    "    \n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors,\n",
    "        to detach them from their history.\"\"\"\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # input: input tensor of shape [seq_len x batch_size]\n",
    "        # hidden: a list of (h_init, c_init), each element has shape (n_layers x num_directions, batch_size, hidden_size)\n",
    "        hidden = None\n",
    "        batch_size = inputs.size(1)\n",
    "        # emb = self.encoder(inputs)\n",
    "        emb = embedded_dropout(self.encoder, inputs, dropout=0.1 if self.training else 0)\n",
    "        emb = self.lockdrop(emb, 0.65)\n",
    "        raw_output = emb\n",
    "        new_hidden = []\n",
    "        for l, rnn in enumerate(self.rnns):\n",
    "            current_input = raw_output\n",
    "            raw_output, new_h = rnn(raw_output, hidden)\n",
    "            new_hidden.append(new_h)\n",
    "            if l != self.n_layers-1:\n",
    "                raw_output = self.lockdrop(raw_output, 0.3)\n",
    "        hidden = new_hidden\n",
    "        output = self.lockdrop(raw_output, 0.4)\n",
    "        # flatten the output\n",
    "        output = output.view(output.size(0)*output.size(1), output.size(2))\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        return output.view(-1, batch_size, self.vocab_size), hidden\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=30, weight_decay=1.2e-6)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.batch_size = self.loader.batch_size\n",
    "        # self.hidden = self.model.init_hidden(batch_size)\n",
    "        self.model = self.model.to(device)\n",
    "        self.criterion = self.criterion.to(device)\n",
    "        \n",
    "        # boom time\n",
    "        torch.backends.cudnn.benchmark=True\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = torch.rand(shape).to(device)\n",
    "        return -Variable(torch.log(-torch.log(U+eps)+eps), requires_grad=False).to(device)\n",
    "\n",
    "    def gumbel_logits(self, logits, temperature=0.8):\n",
    "        y = logits + self.sample_gumbel(logits.size())\n",
    "        return y/temperature\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \n",
    "        \"\"\"\n",
    "        #         U = torch.rand((inputs.size(0), inputs.size(1), self.model.vocab_size)).to(device)\n",
    "        #         U = -Variable(torch.log(-torch.log(U+1e-20)+1e-20), requires_grad=False)\n",
    "        #         U = U.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs, hidden = self.model(inputs)\n",
    "        outputs = self.gumbel_logits(outputs)\n",
    "        #         outputs = (outputs + U)/0.8\n",
    "        loss = self.criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))/(self.batch_size*inputs.size(1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w', encoding=\"utf-8\") as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w', encoding=\"utf-8\") as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        \n",
    "        # generally speaking, it will be faster to decompose the model and\n",
    "        # perform the calculation separately, but since this is a very short \n",
    "        # sequence, I got lazy and just use the forward method.\n",
    "        # same thing happens during generation\n",
    "        inputs = torch.from_numpy(inp.transpose()).long().to(device)\n",
    "        outputs, hidden = model(inputs)\n",
    "        output = outputs[-1]\n",
    "        return output.data.cpu().numpy()\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"\n",
    "        generated = []\n",
    "        inputs = torch.from_numpy(inp.transpose()).long().to(device)\n",
    "        outputs, hidden = model(inputs)\n",
    "        output = outputs[-1]\n",
    "        U = torch.rand(output.size())\n",
    "        U = -torch.log(-torch.log(U+1e-20)+1e-20).to(device)\n",
    "        output = output + U\n",
    "        _, current_word = torch.max(output, dim=1)\n",
    "        generated.append(current_word.data.cpu().numpy())\n",
    "        current_word = current_word.unsqueeze(0)\n",
    "        if forward > 1:\n",
    "            for i in range(forward-1):\n",
    "                outputs, hidden = model(current_word)\n",
    "                output = outputs[0]\n",
    "                U = torch.rand(output.size())\n",
    "                U = -torch.log(-torch.log(U+1e-20)+1e-20).to(device)\n",
    "                output = output + U\n",
    "                _, current_word = torch.max(output, dim=1)\n",
    "                generated.append(current_word.data.cpu().numpy())\n",
    "                current_word = current_word.unsqueeze(0)\n",
    "                # if current_word\n",
    "        return np.array(generated).transpose()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1554784403\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "[WeightDrop(\n",
      "  (module): LSTM(400, 1150)\n",
      "), WeightDrop(\n",
      "  (module): LSTM(1150, 1150)\n",
      "), WeightDrop(\n",
      "  (module): LSTM(1150, 1150)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/10]   Loss: 9.3250\n",
      "[VAL]  Epoch [1/10]   Loss: 6.6466\n",
      "Saving model, predictions and generated output for epoch 0 with NLL: 6.6465964\n",
      "[TRAIN]  Epoch [2/10]   Loss: 8.1464\n",
      "[VAL]  Epoch [2/10]   Loss: 5.9108\n",
      "Saving model, predictions and generated output for epoch 1 with NLL: 5.9108276\n",
      "[TRAIN]  Epoch [3/10]   Loss: 7.6550\n",
      "[VAL]  Epoch [3/10]   Loss: 5.4812\n",
      "Saving model, predictions and generated output for epoch 2 with NLL: 5.481227\n",
      "[TRAIN]  Epoch [4/10]   Loss: 7.3678\n",
      "[VAL]  Epoch [4/10]   Loss: 5.1233\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 5.123341\n",
      "[TRAIN]  Epoch [5/10]   Loss: 7.0492\n",
      "[VAL]  Epoch [5/10]   Loss: 5.0906\n",
      "Saving model, predictions and generated output for epoch 4 with NLL: 5.0905848\n",
      "[TRAIN]  Epoch [6/10]   Loss: 6.8808\n",
      "[VAL]  Epoch [6/10]   Loss: 4.8916\n",
      "Saving model, predictions and generated output for epoch 5 with NLL: 4.891618\n",
      "[TRAIN]  Epoch [7/10]   Loss: 6.7612\n",
      "[VAL]  Epoch [7/10]   Loss: 4.8202\n",
      "Saving model, predictions and generated output for epoch 6 with NLL: 4.8202267\n",
      "[TRAIN]  Epoch [8/10]   Loss: 6.5948\n",
      "[VAL]  Epoch [8/10]   Loss: 4.7456\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 4.7455587\n",
      "[TRAIN]  Epoch [9/10]   Loss: 6.4648\n",
      "[VAL]  Epoch [9/10]   Loss: 4.5586\n",
      "Saving model, predictions and generated output for epoch 8 with NLL: 4.5586166\n",
      "[TRAIN]  Epoch [10/10]   Loss: 6.3952\n",
      "[VAL]  Epoch [10/10]   Loss: 4.5703\n"
     ]
    }
   ],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | successful \" eternal influence to the episode retired on the\n",
      "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = = = = Sub @-@ disguised the\n",
      "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | single did not 370 ( <unk> activity the Sinai Water\n",
      "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | anarchy , but without a declaration , in an instrument\n",
      "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | ( veneration from <unk> , and understated differing risen as\n",
      "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , with ten miles ( ARIA for the eyes to\n",
      "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | from <unk> <unk> Kurt ( 5 . precincts in a\n",
      "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . In 1902 , and Tribble adventures of the Soviets\n",
      "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | seize the forest Commission , the first Soul activity and\n",
      "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | on a young Armored Silver <unk> Advisory Medal of Crown\n",
      "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | 2007 , he did not Timor , he felt that\n",
      "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | an enormous locations riding mates and <unk> <unk> G.I. DS\n",
      "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | with the British sailors and \" Man at the Pam\n",
      "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | May 1989 . It was used on renders the 19th\n",
      "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | June 1910 , the forty 'm the three networks Garcia\n",
      "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | ; the lobby of those form singer instead on his\n",
      "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of <unk> \" Yourself the performer ( open Grain <unk>\n",
      "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | <unk> and pulls amounts of the future spreading partly Doux\n",
      "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ <unk> yard interchanges back poorly marked by papers that\n",
      "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the next year and they are <unk> 7 RAR ,\n",
      "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | away Siberian \" <unk> <unk> . <eol> Many as Ozploitation\n",
      "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | ninth @-@ box as his book <unk> G. Thompson (\n",
      "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | church stands with the following session now after the elements\n",
      "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | France , a instructed 361 the 155 Chandler to eliminate\n",
      "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | assimilated as the very much Racial . The ballads and\n",
      "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | , North , were unstable with a parodied . The\n",
      "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | took a rectangular shells involved with a \" Jon steamboats\n",
      "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = = = = = = = = =\n",
      "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" the Đình the fourth @-@ column and miss the\n",
      "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | from his prototype ( from \" would simply copper smaller\n",
      "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | it preserved in film ironic and fourth @-@ the same\n",
      "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . was also arrives in 1825 . Throughout his families\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
